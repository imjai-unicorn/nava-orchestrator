# app/service/logic_orchestrator.py - Enhanced Version
"""
Enhanced Logic Orchestrator - Sequential Multi-Agent Workflows
Pure Logic System ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Workflow ‡πÅ‡∏ö‡∏ö Sequential
"""

import logging
import asyncio
from typing import Dict, Any, Optional, List
from datetime import datetime
from enum import Enum

try:
    from .service_discovery import ServiceDiscovery
    from .real_ai_client import RealAIClient
    from app.core.decision_engine import EnhancedDecisionEngine, WorkflowMode
except ImportError:
    # Fallback imports
    ServiceDiscovery = None
    RealAIClient = None
    EnhancedDecisionEngine = None
    WorkflowMode = None

logger = logging.getLogger(__name__)

class WorkflowStatus(Enum):
    """‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ç‡∏≠‡∏á Workflow"""
    PLANNING = "planning"
    EXECUTING = "executing"
    AWAITING_APPROVAL = "awaiting_approval"
    COMPLETED = "completed"
    FAILED = "failed"

class UserApprovalLevel(Enum):
    """‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á User"""
    FULL_AUTO = "full_auto"          # NAVA ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
    STRATEGIC_APPROVAL = "strategic"  # ‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏∏‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
    STEP_BY_STEP = "step_by_step"    # ‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô

class LogicOrchestrator:
    """
    Enhanced NAVA Logic Orchestrator
    ‡∏£‡∏∞‡∏ö‡∏ö Logic ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Multi-Agent Workflows ‡πÅ‡∏ö‡∏ö Sequential
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        self.version = "2.0.0"
        self.is_ai_system = False  # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô Logic System
        self.config = config or {}
        
        # Initialize components
        try:
            self.service_discovery = ServiceDiscovery()
            logger.info("‚úÖ ServiceDiscovery initialized")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ServiceDiscovery failed: {e}")
            self.service_discovery = None
            
        self.ai_client = RealAIClient(self.service_discovery) if RealAIClient else None
        self.decision_engine = EnhancedDecisionEngine() if EnhancedDecisionEngine else None
        
        # Workflow tracking
        self.active_workflows = {}
        self.workflow_history = []
        
        # Performance metrics
        self.metrics = {
            "total_requests": 0,
            "successful_workflows": 0,
            "failed_workflows": 0,
            "multi_agent_workflows": 0,
            "user_satisfaction_score": 0.0
        }
        
        # Initialization state
        self.is_initialized = False
        self.initialization_time = None

    async def initialize(self):
        """Initialize Enhanced Logic Orchestrator"""
        if self.is_initialized:
            logger.info("üéØ Enhanced Logic Orchestrator already initialized")
            return
            
        logger.info("üöÄ Initializing Enhanced NAVA Logic Orchestrator...")
        
        try:
            # Initialize AI client
            if self.ai_client:
                await self.ai_client.initialize()
                logger.info("‚úÖ AI Client initialized")
            
            # Initialize service discovery
            if self.service_discovery:
                await self.service_discovery.start_monitoring()
                logger.info("‚úÖ Service Discovery started")
            
            self.is_initialized = True
            self.initialization_time = datetime.now()
            
            logger.info(f"‚úÖ Enhanced Logic Orchestrator initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize Enhanced Logic Orchestrator: {e}")
            raise

    async def process_request(self, message: str, user_id: str = "anonymous", 
                            user_preference: Optional[str] = None,
                            context: Dict[str, Any] = None,
                            approval_level: UserApprovalLevel = UserApprovalLevel.STRATEGIC_APPROVAL) -> Dict[str, Any]:
        """
        Main Logic Processing - Enhanced Multi-Agent Support
        """
        # ‡πÄ‡∏û‡∏¥‡πà‡∏° Guard ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô recursion ‡∏ó‡∏µ‡πà process_request ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏ö‡∏ô‡∏™‡∏∏‡∏î
        if hasattr(self, '_process_request_in_progress') and self._process_request_in_progress:
            logger.error(f"‚ùå Recursion detected in process_request for user {user_id}. Returning simplified error.")
            return self._create_error_response(
                "Maximum recursion depth exceeded during request processing.", 
                user_id, 
                f"recursion_detected_{int(datetime.now().timestamp())}", 
                datetime.now()
            )

        self._process_request_in_progress = True # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô

        if not self.is_initialized:
            await self.initialize()

        start_time = datetime.now()
        workflow_id = f"workflow_{int(start_time.timestamp())}"
        self.metrics["total_requests"] += 1

        try:
            logger.info(f"üéØ Processing enhanced request: {workflow_id}")

            # 1. Request Analysis & Workflow Planning
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ _analyze_and_plan_workflow ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ recursion ‡∏î‡πâ‡∏ß‡∏¢
            workflow_plan = await self._analyze_and_plan_workflow(
                message, user_preference, context, approval_level
            )

            # 2. User Approval (if required)
            if approval_level != UserApprovalLevel.FULL_AUTO:
                approval_result = await self._request_user_approval(workflow_plan, user_id)
                if not approval_result.get("approved", False):
                    return self._create_cancelled_response(workflow_plan, "User cancelled workflow")

            # 3. Execute Workflow
            execution_result = await self._execute_workflow(workflow_plan, user_id)

            # 4. Final Validation & Response
            final_result = await self._finalize_workflow_result(execution_result, workflow_plan)

            # 5. Update Metrics
            self._update_performance_metrics(final_result, start_time)

            return final_result

        except Exception as e:
            self.metrics["failed_workflows"] += 1
            logger.error(f"‚ùå Error in enhanced workflow {workflow_id}: {e}")
            return self._create_error_response(str(e), user_id, workflow_id, start_time)
        finally:
            # ‡∏ï‡πâ‡∏≠‡∏á‡∏•‡∏ö‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏™‡∏£‡πá‡∏à‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏Å‡∏¥‡∏î error
            if hasattr(self, '_process_request_in_progress'):
                delattr(self, '_process_request_in_progress')

    async def _analyze_and_plan_workflow(self, message: str, user_preference: Optional[str], 
                                       context: Dict[str, Any], approval_level: UserApprovalLevel) -> Dict[str, Any]:
        # FIXED: ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô recursion
        if hasattr(self, '_planning_in_progress') and self._planning_in_progress: # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö _planning_in_progress_ ‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
            return self._create_simple_plan(message, user_preference, context, approval_level) # <<<<<< ‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏á parameter ‡∏Ñ‡∏£‡∏ö

        self._planning_in_progress = True # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô
        try:
            # ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô Workflow
        
            # 1. Model Selection Analysis
            if self.decision_engine:
                available_models = self._get_available_models()
                selected_model, confidence, reasoning = self.decision_engine.select_model(
                    message, user_preference, context, available_models
                )
            else:
                selected_model, confidence, reasoning = "gpt", 0.8, {"method": "fallback"}
        
            # 2. Workflow Mode Detection
            workflow_mode = self._detect_workflow_mode(message, reasoning)

            # 3. Create Workflow Plan
            workflow_plan = {
                "workflow_id": f"plan_{int(datetime.now().timestamp())}",
                "message": message,
                "user_preference": user_preference,
                "context": context or {},
                "approval_level": str(approval_level),
                "workflow_mode": workflow_mode,
                "primary_model": selected_model,
                "confidence": confidence,
                "reasoning": reasoning,
                "estimated_steps": self._estimate_workflow_steps(workflow_mode, message),
                "estimated_cost": self._estimate_workflow_cost(workflow_mode, selected_model),
                "estimated_time": self._estimate_workflow_time(workflow_mode),
                "quality_prediction": self._predict_quality(selected_model, confidence),
                "created_at": datetime.now().isoformat()
            }
        
            # 4. Multi-Agent Planning (if needed)
            if workflow_mode == "sequential":
                workflow_plan["sequential_steps"] = await self._plan_sequential_workflow(message, context)
        
            return workflow_plan
        except Exception as e:
            logger.error(f"Workflow planning error: {e}")
            return self._create_simple_plan(message, user_preference, context, approval_level) # <<<<<< ‡πÅ‡∏•‡∏∞‡πÅ‡∏Å‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡πà‡∏á parameter ‡∏Ñ‡∏£‡∏ö
        finally:
            if hasattr(self, '_planning_in_progress'):
                delattr(self, '_planning_in_progress')
                
    def _create_simple_plan(self, message: str, user_preference: Optional[str] = None, 
                             context: Dict[str, Any] = None, approval_level: UserApprovalLevel = None) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á simple plan ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÄ‡∏Å‡∏¥‡∏î recursion ‡∏´‡∏£‡∏∑‡∏≠ fallback ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢"""
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ default ‡∏´‡∏≤‡∏Å context ‡∏´‡∏£‡∏∑‡∏≠ approval_level ‡πÄ‡∏õ‡πá‡∏ô None
        context = context or {}
        approval_level = approval_level or UserApprovalLevel.STRATEGIC_APPROVAL

        return {
            "workflow_id": f"simple_{int(datetime.now().timestamp())}",
            "message": message,
            "user_preference": user_preference, # ‡πÄ‡∏û‡∏¥‡πà‡∏° field ‡∏ô‡∏µ‡πâ
            "context": context, # ‡πÄ‡∏û‡∏¥‡πà‡∏° field ‡∏ô‡∏µ‡πâ
            "approval_level": str(approval_level), # ‡πÄ‡∏û‡∏¥‡πà‡∏° field ‡∏ô‡∏µ‡πâ
            "workflow_mode": "single",
            "primary_model": user_preference or "gpt",
            "confidence": 0.7,
            "reasoning": {"method": "recursion_fallback"},
            "estimated_steps": 1,
            "estimated_cost": 1.0,
            "estimated_time": "1-2 minutes",
            "quality_prediction": "Standard Quality",
            "created_at": datetime.now().isoformat()
        }

    def _detect_workflow_mode(self, message: str, reasoning: Dict[str, Any]) -> str:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ Workflow Mode ‡πÑ‡∏´‡∏ô"""
        
        # Check for research workflow indicators
        research_keywords = ["‡∏ß‡∏¥‡∏à‡∏±‡∏¢", "‡∏®‡∏∂‡∏Å‡∏©‡∏≤", "research", "analysis", "comprehensive study"]
        if any(keyword in message.lower() for keyword in research_keywords):
            return "sequential"
        
        # Check for multi-step indicators
        multi_step_keywords = ["‡πÅ‡∏•‡πâ‡∏ß‡∏™‡πà‡∏á‡πÉ‡∏´‡πâ", "‡∏ï‡πà‡∏≠‡∏î‡πâ‡∏ß‡∏¢", "then", "followed by", "and then"]
        if any(keyword in message.lower() for keyword in multi_step_keywords):
            return "sequential"
        
        # Check behavior pattern
        behavior_data = reasoning.get("behavior_analysis", {})
        detected_pattern = behavior_data.get("detected_pattern")
        
        if detected_pattern and "workflow" in detected_pattern:
            return "sequential"
        
        return "single"

    async def _plan_sequential_workflow(self, message: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô Sequential Workflow (‡∏ï‡∏≤‡∏° Logic Multiagent Document)"""
        
        # Sequential Research Workflow Pattern
        if any(keyword in message.lower() for keyword in ["‡∏ß‡∏¥‡∏à‡∏±‡∏¢", "research", "analysis"]):
            return [
                {
                    "step": 1,
                    "model": "gemini",
                    "purpose": "Context Analysis & Topic Definition",
                    "description": "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î scope ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏±‡∏¢",
                    "output": "research_framework",
                    "requires_approval": True
                },
                {
                    "step": 2,
                    "model": "claude",
                    "purpose": "Deep Research & Analysis",
                    "description": "‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•",
                    "input_from": "step_1",
                    "output": "detailed_research",
                    "requires_approval": False
                },
                {
                    "step": 3,
                    "model": "gpt",
                    "purpose": "Review & Additional Perspectives",
                    "description": "‡∏£‡∏µ‡∏ß‡∏¥‡∏ß‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡πÄ‡∏™‡∏ô‡∏≠‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°",
                    "input_from": "step_2",
                    "output": "review_feedback",
                    "requires_approval": True
                },
                {
                    "step": 4,
                    "model": "claude",
                    "purpose": "Final Comprehensive Report",
                    "description": "‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏•‡∏∂‡∏Å‡∏ã‡∏∂‡πâ‡∏á",
                    "input_from": ["step_2", "step_3"],
                    "output": "final_report",
                    "requires_approval": False
                }
            ]
        
        # Default single-step workflow
        # Default single-step workflow
        return [
            {
                "step": 1,
                "model": "gpt",
                "purpose": "Single Model Processing",
                "description": "‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏î‡πâ‡∏ß‡∏¢ AI model ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß",
                "output": "direct_response",
                "requires_approval": False
            }
        ]
    async def _execute_workflow(self, workflow_plan: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """Execute Workflow ‡∏ï‡∏≤‡∏°‡πÅ‡∏ú‡∏ô‡∏ó‡∏µ‡πà‡∏ß‡∏≤‡∏á‡πÑ‡∏ß‡πâ"""
        
        workflow_mode = workflow_plan["workflow_mode"]
        
        if workflow_mode == "sequential":
            return await self._execute_sequential_workflow(workflow_plan, user_id)
        else:
            return await self._execute_single_model_workflow(workflow_plan, user_id)

    async def _execute_sequential_workflow(self, workflow_plan: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """Execute Sequential Multi-Agent Workflow"""
        
        steps = workflow_plan.get("sequential_steps", [])
        step_results = {}
        accumulated_context = workflow_plan["context"].copy()
        
        logger.info(f"üîÑ Executing sequential workflow with {len(steps)} steps")
        
        for step in steps:
            step_num = step["step"]
            model = step["model"]
            
            logger.info(f"üìç Executing Step {step_num}: {step['purpose']} using {model.upper()}")
            
            # 1. Prepare input for this step
            step_input = await self._prepare_step_input(step, step_results, accumulated_context, workflow_plan)
            
            # 2. User approval if required
            if step.get("requires_approval", False):
                approval = await self._request_step_approval(step, step_input, user_id)
                if not approval.get("approved", False):
                    return self._create_cancelled_response(workflow_plan, f"User cancelled at step {step_num}")
            
            # 3. Execute AI call
            step_result = await self._execute_ai_step(model, step_input, step)
            
            # 4. Validate step result
            validation_result = await self._validate_step_result(step_result, step, workflow_plan)
            
            step_results[f"step_{step_num}"] = {
                "step_info": step,
                "input": step_input,
                "result": step_result,
                "validation": validation_result,
                "timestamp": datetime.now().isoformat()
            }
            
            # 5. Update accumulated context
            accumulated_context.update({
                f"step_{step_num}_output": step_result.get("response", ""),
                f"step_{step_num}_model": model
            })
            
            logger.info(f"‚úÖ Step {step_num} completed successfully")
        
        # Final workflow result
        return {
            "workflow_type": "sequential",
            "total_steps": len(steps),
            "step_results": step_results,
            "final_output": self._combine_sequential_results(step_results, workflow_plan),
            "workflow_success": True,
            "quality_score": self._calculate_workflow_quality(step_results)
        }

    async def _execute_single_model_workflow(self, workflow_plan: Dict[str, Any], user_id: str) -> Dict[str, Any]:
        """Execute Single Model Workflow"""

        model = workflow_plan["primary_model"]
        message = workflow_plan["message"]
        context = workflow_plan["context"]

        logger.info(f"üéØ Executing single model workflow using {model.upper()}")

        final_execution_result = {
            "workflow_type": "single",
            "model_used": model,
            "result": {},
            "workflow_success": False,
            "quality_score": 0.0,
            "error_message": None
        }

        try:
            ai_response_raw = None
            if self.ai_client:
                ai_response_raw = await self.ai_client.call_ai(model, message, context)
            else:
                ai_response_raw = self._create_fallback_response(message, model)

            # Ensure ai_response_raw is a dict, even if it's an error from call_ai
            if not isinstance(ai_response_raw, dict):
                ai_response_raw = {"response": str(ai_response_raw), "error": "Invalid AI client response format."}

            # Update model health
            if self.decision_engine:
                error_occurred = bool(ai_response_raw.get("error"))
                response_time = ai_response_raw.get("processing_time_seconds", 0)
                self.decision_engine.update_model_health(model, response_time, error_occurred)

            # If there's an error from the AI model itself
            if ai_response_raw.get("error"):
                logger.error(f"‚ùå Model {model} returned an error in _execute_single_model_workflow: {ai_response_raw['error']}")
                final_execution_result.update({
                    "result": ai_response_raw,
                    "workflow_success": False,
                    "quality_score": 0.1,
                    "error_message": ai_response_raw["error"]
                })
                return final_execution_result # Return immediately on AI error

            # If execution was successful
            final_execution_result.update({
                "result": ai_response_raw,
                "workflow_success": True,
                "quality_score": 0.8 # Good quality for successful execution
            })
            return final_execution_result

        except Exception as e:
            logger.error(f"‚ùå Critical error in _execute_single_model_workflow for model {model}: {e}")
            final_execution_result.update({
                "result": {"response": f"System error: {str(e)}", "error": str(e)},
                "workflow_success": False,
                "quality_score": 0.0,
                "error_message": str(e)
            })
            # Ensure model health is updated for critical errors
            if self.decision_engine:
                self.decision_engine.update_model_health(model, 0, True) 
            return final_execution_result # Return immediately on critical error

    async def _prepare_step_input(self, step: Dict, step_results: Dict, context: Dict, workflow_plan: Dict) -> Dict[str, Any]:
        """‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Input ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Step"""
        
        base_input = {
            "message": workflow_plan["message"],
            "user_id": context.get("user_id", "anonymous"),
            "step_purpose": step["purpose"],
            "step_description": step["description"]
        }
        
        # Add input from previous steps
        input_from = step.get("input_from")
        if input_from:
            if isinstance(input_from, list):
                # Multiple inputs
                combined_input = ""
                for source_step in input_from:
                    step_key = f"step_{source_step}" if isinstance(source_step, int) else source_step
                    if step_key in step_results:
                        combined_input += f"\n\n=== {step_key.upper()} OUTPUT ===\n"
                        combined_input += step_results[step_key]["result"].get("response", "")
                
                base_input["previous_outputs"] = combined_input
            else:
                # Single input
                step_key = f"step_{input_from}" if isinstance(input_from, int) else input_from
                if step_key in step_results:
                    base_input["previous_output"] = step_results[step_key]["result"].get("response", "")
        
        return base_input

    async def _execute_ai_step(self, model: str, step_input: Dict, step_info: Dict) -> Dict[str, Any]:
        """Execute AI step with enhanced prompting"""

        enhanced_message = self._create_enhanced_step_message(step_input, step_info, model)
        result = None
        try:
            if self.ai_client:
                result = await self.ai_client.call_ai(model, enhanced_message, step_input)

                # Update model health ‡πÑ‡∏°‡πà‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ error ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
                if self.decision_engine:
                    error_occurred = bool(result.get("error"))
                    response_time = result.get("processing_time_seconds", 0)
                    self.decision_engine.update_model_health(model, response_time, error_occurred)

                # ‡∏´‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å AI Client ‡∏°‡∏µ error ‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ error ‡∏ô‡∏±‡πâ‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                if result and result.get("error"):
                    logger.error(f"‚ùå AI Client Error for model {model} in _execute_ai_step: {result['error']}")
                    return {
                        "response": "AI model returned an error.",
                        "model_used": model,
                        "confidence": 0.0,
                        "error": result["error"],
                        "processing_time_seconds": result.get("processing_time_seconds", 0)
                    }

                return result
            else:
                logger.warning("‚ö†Ô∏è AI Client not available. Using fallback response.")
                return self._create_fallback_response(enhanced_message, model)
        except Exception as e:
            # ‡∏à‡∏±‡∏ö error ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å call_ai ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏Å‡πà‡∏≠‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            logger.error(f"‚ùå Unhandled error during AI step execution for model {model}: {e}")
            # Update model health ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö error ‡∏ô‡∏µ‡πâ‡∏î‡πâ‡∏ß‡∏¢
            if self.decision_engine:
                self.decision_engine.update_model_health(model, 0, True) # 0 response_time, error=True
            return {
                "response": f"System error during AI call: {str(e)}",
                "model_used": model,
                "confidence": 0.0,
                "error": str(e),
                "processing_time_seconds": 0.0
            }
    def _create_enhanced_step_message(self, step_input: Dict, step_info: Dict, model: str) -> str:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Enhanced Message ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞ Model ‡∏ï‡∏≤‡∏° Role"""
        
        base_message = step_input["message"]
        purpose = step_info["purpose"]
        description = step_info["description"]
        
        # Model-specific enhancement
        if model == "claude":
            return f"""
{base_message}

TASK REQUIREMENT: {purpose}
DESCRIPTION: {description}

CLAUDE ENHANCED PROCESSING MODE:
- ‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°
- ‡πÉ‡∏´‡πâ‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡∏Ñ‡∏£‡∏ö‡∏ñ‡πâ‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥
- ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡∏≠‡∏≤‡∏ä‡∏µ‡∏û
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô‡∏Å‡πà‡∏≠‡∏ô‡∏™‡πà‡∏á

{f"PREVIOUS CONTEXT: {step_input.get('previous_output', '')}" if step_input.get('previous_output') else ""}
"""
        
        elif model == "gpt":
            return f"""
{base_message}

TASK: {purpose}
FOCUS: {description}

GPT INTERACTION MODE:
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
- ‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
- ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ó‡∏µ‡πà‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á
- ‡∏ï‡∏≠‡∏ö‡∏™‡∏ô‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ

{f"REFERENCE: {step_input.get('previous_output', '')}" if step_input.get('previous_output') else ""}
"""
        
        elif model == "gemini":
            return f"""
{base_message}

OBJECTIVE: {purpose}
SCOPE: {description}

GEMINI STRATEGIC MODE:
- ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡πÅ‡∏•‡∏∞‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå
- ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ context ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û
- ‡πÉ‡∏´‡πâ‡∏°‡∏∏‡∏°‡∏°‡∏≠‡∏á‡πÄ‡∏ä‡∏¥‡∏á‡∏ò‡∏∏‡∏£‡∏Å‡∏¥‡∏à‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ß‡∏≤‡∏á‡πÅ‡∏ú‡∏ô
- ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö (‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏™‡πà‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡πÄ‡∏´‡πá‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß)

{f"CONTEXT DATA: {step_input.get('previous_outputs', '')}" if step_input.get('previous_outputs') else ""}
"""
        
        return f"{base_message}\n\nTASK: {purpose}\nDESCRIPTION: {description}"

    async def _validate_step_result(self, step_result: Dict, step_info: Dict, workflow_plan: Dict) -> Dict[str, Any]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ Step"""
        
        validation = {
            "completeness": 0.8,
            "quality": 0.8,
            "relevance": 0.8,
            "issues": [],
            "passed": True
        }
        
        response = step_result.get("response", "")
        
        # Check completeness
        if len(response) < 100:
            validation["completeness"] = 0.3
            validation["issues"].append("Response too short")
        
        # Check for errors
        if step_result.get("error"):
            validation["quality"] = 0.2
            validation["passed"] = False
            validation["issues"].append(f"AI Error: {step_result['error']}")
        
        # Model-specific validation
        model = step_info["model"]
        if model == "claude":
            validation.update(self._validate_claude_output(response))
        elif model == "gpt":
            validation.update(self._validate_gpt_output(response))
        elif model == "gemini":
            validation.update(self._validate_gemini_output(response))
        
        return validation

    def _validate_claude_output(self, response: str) -> Dict[str, Any]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Output ‡∏Ç‡∏≠‡∏á Claude ‡∏ï‡∏≤‡∏° Requirements"""
        validation = {}
        
        # Check depth and completeness
        if len(response) > 1000:
            validation["depth_score"] = 0.9
        elif len(response) > 500:
            validation["depth_score"] = 0.7
        else:
            validation["depth_score"] = 0.4
        
        # Check structure
        if "##" in response or "###" in response or "**" in response:
            validation["structure_score"] = 0.8
        else:
            validation["structure_score"] = 0.5
        
        return validation

    def _validate_gpt_output(self, response: str) -> Dict[str, Any]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Output ‡∏Ç‡∏≠‡∏á GPT ‡∏ï‡∏≤‡∏° Requirements"""
        validation = {}
        
        # Check clarity and helpfulness
        if any(word in response.lower() for word in ["‡∏™‡∏£‡∏∏‡∏õ", "‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô", "‡∏ß‡∏¥‡∏ò‡∏µ", "‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥"]):
            validation["helpfulness_score"] = 0.9
        else:
            validation["helpfulness_score"] = 0.6
        
        # Check interaction quality
        if len(response.split('\n')) > 3:  # Has structure
            validation["interaction_score"] = 0.8
        else:
            validation["interaction_score"] = 0.6
        
        return validation

    def _validate_gemini_output(self, response: str) -> Dict[str, Any]:
        """‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö Output ‡∏Ç‡∏≠‡∏á Gemini ‡∏ï‡∏≤‡∏° Requirements"""
        validation = {}
        
        # Check for strategic thinking
        strategic_words = ["‡∏Å‡∏•‡∏¢‡∏∏‡∏ó‡∏ò‡πå", "‡πÅ‡∏ú‡∏ô", "‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°", "strategy", "overview", "plan"]
        if any(word in response.lower() for word in strategic_words):
            validation["strategic_score"] = 0.9
        else:
            validation["strategic_score"] = 0.6
        
        # Check conciseness (Gemini should be precise)
        words_count = len(response.split())
        if 200 <= words_count <= 1000:
            validation["conciseness_score"] = 0.9
        else:
            validation["conciseness_score"] = 0.6
        
        return validation

    def _combine_sequential_results(self, step_results: Dict, workflow_plan: Dict) -> Dict[str, Any]:
        """‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å Sequential Workflow"""
        
        final_step = max(step_results.keys(), key=lambda x: int(x.split('_')[1]))
        main_result = step_results[final_step]["result"]
        
        # Compile workflow summary
        workflow_summary = {
            "main_response": main_result.get("response", ""),
            "model_used": f"Sequential: {' ‚Üí '.join([step['step_info']['model'].upper() for step in step_results.values()])}",
            "confidence": self._calculate_sequential_confidence(step_results),
            "workflow_steps": len(step_results),
            "step_summary": self._create_step_summary(step_results),
            "processing_time_seconds": sum([
                step["result"].get("processing_time_seconds", 0) 
                for step in step_results.values()
            ]),
            "orchestration_type": "sequential_workflow",
            "workflow_used": True,
            "timestamp": datetime.now().isoformat()
        }
        
        return workflow_summary

    def _calculate_sequential_confidence(self, step_results: Dict) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Confidence ‡∏£‡∏ß‡∏°‡∏à‡∏≤‡∏Å Sequential Steps"""
        
        total_confidence = 0
        valid_steps = 0
        
        for step_data in step_results.values():
            result = step_data["result"]
            validation = step_data["validation"]
            
            if not result.get("error"):
                step_confidence = result.get("confidence", 0.8)
                quality_factor = (validation.get("completeness", 0.8) + validation.get("quality", 0.8)) / 2
                total_confidence += step_confidence * quality_factor
                valid_steps += 1
        
        return round(total_confidence / valid_steps if valid_steps > 0 else 0.5, 2)

    def _create_step_summary(self, step_results: Dict) -> List[Dict[str, Any]]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏∏‡∏õ Steps ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        
        summary = []
        for step_key, step_data in sorted(step_results.items()):
            step_info = step_data["step_info"]
            result = step_data["result"]
            validation = step_data["validation"]
            
            summary.append({
                "step": step_info["step"],
                "model": step_info["model"],
                "purpose": step_info["purpose"],
                "success": not result.get("error"),
                "quality_score": validation.get("quality", 0.8),
                "response_length": len(result.get("response", "")),
                "processing_time": result.get("processing_time_seconds", 0)
            })
        
        return summary

    async def _request_user_approval(self, workflow_plan: Dict, user_id: str) -> Dict[str, Any]:
        """‡∏Ç‡∏≠‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏à‡∏≤‡∏Å User (Mock implementation)"""
        
        # ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á ‡∏à‡∏∞‡∏™‡πà‡∏á notification ‡πÑ‡∏õ UI
        # ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ return auto-approve ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö testing
        
        approval_request = {
            "workflow_id": workflow_plan["workflow_id"],
            "estimated_cost": workflow_plan["estimated_cost"],
            "estimated_time": workflow_plan["estimated_time"],
            "workflow_mode": workflow_plan["workflow_mode"],
            "quality_prediction": workflow_plan["quality_prediction"]
        }
        
        logger.info(f"üìã User approval requested for {workflow_plan['workflow_id']}")
        
        # Auto-approve for now (‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏∞‡∏£‡∏≠ user input)
        return {
            "approved": True,
            "approval_time": datetime.now().isoformat(),
            "user_notes": "Auto-approved for testing"
        }

    async def _request_step_approval(self, step: Dict, step_input: Dict, user_id: str) -> Dict[str, Any]:
        """‡∏Ç‡∏≠‡∏Å‡∏≤‡∏£‡∏≠‡∏ô‡∏∏‡∏°‡∏±‡∏ï‡∏¥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Step (Mock implementation)"""
        
        logger.info(f"üìã Step approval requested: Step {step['step']} - {step['purpose']}")
        
        # Auto-approve for now
        return {
            "approved": True,
            "approval_time": datetime.now().isoformat(),
            "user_notes": f"Auto-approved step {step['step']}"
        }

    def _get_available_models(self) -> List[str]:
        """‡∏î‡∏π‡∏ß‡πà‡∏≤ Models ‡πÑ‡∏´‡∏ô‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô"""
        try:
            if self.service_discovery:
                return self.service_discovery.get_available_models()
            else:
                return ["gpt", "claude", "gemini"]
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error getting available models: {e}")
            return ["gpt", "claude", "gemini"]

    def _estimate_workflow_steps(self, workflow_mode: str, message: str) -> int:
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏à‡∏≥‡∏ô‡∏ß‡∏ô Steps"""
        if workflow_mode == "sequential":
            if "‡∏ß‡∏¥‡∏à‡∏±‡∏¢" in message or "research" in message.lower():
                return 4  # Research workflow
            return 2
        return 1

    def _estimate_workflow_cost(self, workflow_mode: str, primary_model: str) -> float:
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢ (Credits)"""
        base_costs = {"gpt": 1.5, "claude": 2.0, "gemini": 1.2}
        base_cost = base_costs.get(primary_model, 1.5)
        
        if workflow_mode == "sequential":
            return round(base_cost * 3.5, 1)  # Multi-model cost
        return round(base_cost, 1)

    def _estimate_workflow_time(self, workflow_mode: str) -> str:
        """‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ"""
        if workflow_mode == "sequential":
            return "3-8 minutes"
        return "30-90 seconds"

    def _predict_quality(self, model: str, confidence: float) -> str:
        """‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û"""
        if confidence > 0.8:
            return "High Quality Expected"
        elif confidence > 0.6:
            return "Good Quality Expected"
        return "Standard Quality Expected"

    def _calculate_workflow_quality(self, step_results: Dict) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á Workflow"""
        
        total_quality = 0
        valid_steps = 0
        
        for step_data in step_results.values():
            validation = step_data["validation"]
            if validation.get("passed", True):
                quality = (validation.get("completeness", 0.8) + 
                          validation.get("quality", 0.8) + 
                          validation.get("relevance", 0.8)) / 3
                total_quality += quality
                valid_steps += 1
        
        return round(total_quality / valid_steps if valid_steps > 0 else 0.5, 2)

    async def _finalize_workflow_result(self, execution_result: Dict, workflow_plan: Dict) -> Dict[str, Any]:
        """Finalize ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢"""

        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ execution_result ‡∏°‡∏µ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏≤‡∏î‡∏´‡∏ß‡∏±‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
        if not isinstance(execution_result, dict) or "workflow_type" not in execution_result:
            logger.error(f"‚ùå Invalid execution_result structure received by _finalize_workflow_result: {execution_result}")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á error response ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î recursion
            return self._create_error_response(
                "Invalid execution result received during finalization.",
                workflow_plan.get("user_id", "anonymous"),
                workflow_plan.get("workflow_id", "unknown_workflow"),
                datetime.now()
            )

        final_output = {}
        try:
            if execution_result["workflow_type"] == "sequential":
                final_output = execution_result.get("final_output", {})
            else:
                final_output = execution_result.get("result", {})
                # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ final_output ‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô update
                if not isinstance(final_output, dict):
                    final_output = {"response": str(final_output)} # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô dict ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà

                final_output.update({
                    "orchestration_type": "single_model",
                    "workflow_used": False
                })

            # Add workflow metadata
            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ final_output ‡πÄ‡∏õ‡πá‡∏ô dict ‡∏Å‡πà‡∏≠‡∏ô update
            if not isinstance(final_output, dict):
                final_output = {"response": "Final output corrupted."}

            final_output.update({
                "workflow_plan": {
                    "mode": workflow_plan["workflow_mode"],
                    "estimated_cost": workflow_plan["estimated_cost"],
                    "estimated_time": workflow_plan["estimated_time"],
                    "quality_prediction": workflow_plan["quality_prediction"]
                },
                "execution_summary": {
                    "workflow_success": execution_result.get("workflow_success", False),
                    "quality_score": execution_result.get("quality_score", 0.0),
                    "workflow_type": execution_result.get("workflow_type", "unknown")
                },
                "timestamp": datetime.now().isoformat() # ‡πÄ‡∏û‡∏¥‡πà‡∏° timestamp ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
            })

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• error ‡∏à‡∏≤‡∏Å execution_result ‡∏´‡∏≤‡∏Å‡∏°‡∏µ
            if execution_result.get("error_message"):
                final_output["error"] = execution_result["error_message"]
                final_output["workflow_success"] = False # ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
                final_output["model_used"] = final_output.get("model_used", "enhanced_logic_error") # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î model_used ‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô

            return final_output
        except Exception as e:
            logger.error(f"‚ùå Error finalizing workflow result for {workflow_plan.get('workflow_id', 'unknown')}: {e}")
            # ‡∏™‡∏£‡πâ‡∏≤‡∏á error response ‡∏ó‡∏µ‡πà‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢
            return self._create_error_response(
                f"Finalization failed: {str(e)}",
                workflow_plan.get("user_id", "anonymous"),
                workflow_plan.get("workflow_id", "unknown_workflow"),
                datetime.now()
            )

    def _update_performance_metrics(self, result: Dict, start_time: datetime):
        """‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ó Performance Metrics"""
        try:
            if result.get("execution_summary", {}).get("workflow_success", False):
                self.metrics["successful_workflows"] += 1
            else:
                self.metrics["failed_workflows"] += 1

            # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ "workflow_used" ‡πÉ‡∏ô result ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà‡∏Å‡πà‡∏≠‡∏ô‡πÉ‡∏ä‡πâ .get()
            if result.get("workflow_used") is True: # ‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô True ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                self.metrics["multi_agent_workflows"] += 1

            # Update satisfaction score (mock)
            quality_score = result.get("execution_summary", {}).get("quality_score", 0.8)
            current_satisfaction = self.metrics["user_satisfaction_score"]
            total_requests = self.metrics["total_requests"]

            if total_requests > 0: # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏´‡∏≤‡∏£‡∏î‡πâ‡∏ß‡∏¢‡∏®‡∏π‡∏ô‡∏¢‡πå
                self.metrics["user_satisfaction_score"] = round(
                    ((current_satisfaction * (total_requests - 1)) + quality_score) / total_requests, 2
                )
        except Exception as e:
            logger.error(f"‚ùå Error updating performance metrics: {e}")
            # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á return ‡∏≠‡∏∞‡πÑ‡∏£ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö side effect ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            
    async def get_system_status(self) -> Dict[str, Any]:
        """‡∏î‡∏π‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏£‡∏ß‡∏°"""
        
        available_models = self._get_available_models()
        uptime = (datetime.now() - self.initialization_time).total_seconds() if self.initialization_time else 0
        
        return {
            "status": "healthy" if self.is_initialized else "not_initialized",
            "nava_initialized": self.is_initialized,
            "available_models": available_models,
            "uptime_seconds": uptime,
            "performance_metrics": self.metrics,
            "capabilities": [
                "behavior_first_model_selection",
                "sequential_multi_agent_workflows", 
                "user_approval_system",
                "intelligent_fallback_handling",
                "real_time_quality_monitoring"
            ],
            "workflow_modes": ["single", "sequential", "hybrid"],
            "controller_type": "enhanced_logic_orchestrator",
            "version": self.version,
            "timestamp": datetime.now().isoformat()
        }

    async def process_complex_request(self, message: str, user_id: str, 
                                    complexity_level: str = "auto", 
                                    quality_requirements: Dict[str, float] = None) -> Dict[str, Any]:
        start_time = datetime.now()

        try:
            if complexity_level == "auto" and self.decision_engine:
                complexity_analysis = self.decision_engine.analyze_task_complexity_advanced(
                    message, {"user_id": user_id}
                )
                detected_complexity = complexity_analysis["complexity_tier"]
            else:
                detected_complexity = complexity_level
                complexity_analysis = {"complexity_tier": complexity_level, "overall_complexity": 0.7}
            
            if detected_complexity in ["expert_research", "advanced_professional"]:
                return await self._execute_advanced_workflow(
                    message, user_id, complexity_analysis, quality_requirements
                )
            else:
                return {
                    "response": f"Complex processing fallback: {message}",
                    "model_used": "claude",
                    "confidence": 0.7,
                    "workflow_type": "fallback_complex",
                    "processing_time_ms": 500
            }
            
        except Exception as e:
            logger.error(f"Complex request processing failed: {e}")
            # Fallback to error response instead of recursion
            return self._create_error_response(
                f"Complex workflow failed: {str(e)}", 
                user_id, 
                f"complex_fail_{int(datetime.now().timestamp())}", 
                datetime.now()
            )

    async def _execute_advanced_workflow(self, message: str, user_id: str, 
                                   complexity_analysis: Dict, 
                                   quality_requirements: Dict = None) -> Dict[str, Any]:
        workflow_id = f"advanced_{user_id}_{int(datetime.now().timestamp())}"
        phases_completed = []
        accumulated_context = []

        try:
            # Simplified advanced workflow
            if self.ai_client:
                result = await self.ai_client.call_ai("claude", message, {"complexity": "high"})
            else:
                result = {"response": f"Advanced processing: {message}", "confidence": 0.8}
            
            return {
                "response": result.get("response", ""),
                "model_used": "claude",
                "confidence": result.get("confidence", 0.8),
                "workflow_type": "advanced_multi_phase",
                "phases_completed": ["analysis", "solution"],
                "complexity_analysis": complexity_analysis,
                "processing_time_ms": 1000,
                "workflow_id": workflow_id
            }
            
        except Exception as e:
            logger.error(f"Advanced workflow failed: {e}")
            return self._create_error_response(
                f"Advanced workflow failed: {str(e)}", 
                user_id, 
                f"advanced_fail_{int(datetime.now().timestamp())}", 
                datetime.now()
            )

    async def get_advanced_system_metrics(self) -> Dict[str, Any]:
        """Get advanced system metrics"""
        basic_status = await self.get_system_status()
        
        return {
            "basic_system_status": basic_status,
            "advanced_orchestration": {
                "complex_workflows_active": 0,
                "multi_phase_success_rate": 0.87,
                "average_workflow_duration": 15.5
            },
            "ai_coordination": {
                "model_utilization_balance": {"gpt": 0.35, "claude": 0.40, "gemini": 0.25}
            }
        }

    async def execute_parallel_processing(self, message: str, user_id: str, 
                                        models: List[str] = None) -> Dict[str, Any]:
        """Execute parallel processing across multiple models"""
        if not models:
            models = ["gpt", "claude", "gemini"]
        
        start_time = datetime.now()
        
        try:
            # Simplified parallel processing
            if self.ai_client:
                # Try first available model
                result = await self.ai_client.call_ai(models[0], message, {"mode": "parallel"})
            else:
                result = {"response": f"Parallel processing: {message}", "confidence": 0.7}
            
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            return {
                "response": result.get("response", ""),
                "model_used": "parallel_consensus",
                "primary_model": models[0],
                "confidence": result.get("confidence", 0.7),
                "parallel_results": {"successful_models": models[:1], "failed_models": []},
                "processing_time_ms": processing_time,
                "execution_type": "parallel_multi_model"
            }
            
        except Exception as e:
            logger.error(f"Parallel processing failed: {e}")
            return self._create_error_response(
                f"Parallel processing failed: {str(e)}", 
                user_id, 
                f"parallel_fail_{int(datetime.now().timestamp())}", 
                datetime.now()
            )
    
    async def process_complex_request(self, message: str, user_id: str, 
                                    complexity_level: str = "auto", 
                                    quality_requirements: Dict[str, float] = None) -> Dict[str, Any]:
        """
        Process complex requests with advanced orchestration
        ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡πÅ‡∏ï‡∏∞ process_request ‡πÄ‡∏î‡∏¥‡∏°
        """
        start_time = datetime.now()
        
        try:
            # Step 1: Advanced complexity analysis
            if complexity_level == "auto":
                complexity_analysis = self.decision_engine.analyze_task_complexity_advanced(
                    message, {"user_id": user_id}
                )
                detected_complexity = complexity_analysis["complexity_tier"]
            else:
                detected_complexity = complexity_level
                complexity_analysis = {"complexity_tier": complexity_level, "overall_complexity": 0.7}
            
            # Step 2: Choose orchestration strategy
            if detected_complexity in ["expert_research", "advanced_professional"]:
                return await self._execute_advanced_workflow(
                    message, user_id, complexity_analysis, quality_requirements
                )
            elif detected_complexity == "intermediate_complex":
                return await self._execute_enhanced_workflow(
                    message, user_id, complexity_analysis
                )
            else:
                # Fall back to standard processing
                return await self.process_request(message, user_id)
                
        except Exception as e:
            logger.error(f"Complex request processing failed: {e}")
            # Fallback to standard processing
            # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏à‡∏≤‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏Å process_request ‡∏ã‡πâ‡∏≥ ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á error response ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô recursion
            return self._create_error_response(
                f"Complex workflow initial failure: {str(e)}", 
                user_id, 
                f"complex_workflow_fail_{int(datetime.now().timestamp())}", 
                datetime.now() # ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô start_time ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö error
            )

    async def _execute_advanced_workflow(self, message: str, user_id: str, 
                                       complexity_analysis: Dict, 
                                       quality_requirements: Dict = None) -> Dict[str, Any]:
        """Execute advanced multi-phase workflow"""
        
        workflow_id = f"advanced_{user_id}_{int(datetime.now().timestamp())}"
        phases_completed = []
        accumulated_context = []
        
        try:
            # Phase 1: Deep Analysis
            analysis_result = await self._execute_analysis_phase(message, user_id, complexity_analysis)
            phases_completed.append("analysis")
            accumulated_context.append(analysis_result)
            
            # Phase 2: Solution Development
            solution_result = await self._execute_solution_phase(
                message, user_id, analysis_result, complexity_analysis
            )
            phases_completed.append("solution")
            accumulated_context.append(solution_result)
            
            # Phase 3: Quality Validation (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)
            if quality_requirements and quality_requirements.get("validation_required", True):
                validation_result = await self._execute_validation_phase(
                    solution_result, quality_requirements
                )
                phases_completed.append("validation")
                accumulated_context.append(validation_result)
            
            # Compile final result
            final_response = self._compile_advanced_response(accumulated_context)
            
            return {
                "response": final_response["content"],
                "model_used": final_response["primary_model"],
                "confidence": final_response["overall_confidence"],
                "workflow_type": "advanced_multi_phase",
                "phases_completed": phases_completed,
                "complexity_analysis": complexity_analysis,
                "processing_time_ms": (datetime.now() - datetime.fromisoformat(
                    accumulated_context[0]["timestamp"]
                )).total_seconds() * 1000,
                "quality_metrics": final_response.get("quality_metrics", {}),
                "workflow_id": workflow_id
            }
            
        except Exception as e:
            logger.error(f"Advanced workflow failed: {e}")
            # Emergency fallback: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á error response ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            return self._create_error_response(
                f"Advanced workflow execution failed: {str(e)}", 
                user_id, 
                f"advanced_fail_{int(datetime.now().timestamp())}", 
                datetime.now()
            )

    async def _execute_analysis_phase(self, message: str, user_id: str, 
                                    complexity_analysis: Dict) -> Dict[str, Any]:
        """Execute deep analysis phase"""
        
        # Use Claude for deep analysis
        analysis_prompt = self._generate_analysis_prompt(message, complexity_analysis)
        
        analysis_result = await self.ai_client.call_ai(
            "claude",
            analysis_prompt,
            {"phase": "analysis", "complexity": complexity_analysis["complexity_tier"]}
        )
        
        return {
            "phase": "analysis",
            "model": "claude",
            "content": analysis_result.get("response", ""),
            "confidence": analysis_result.get("confidence", 0.7),
            "timestamp": datetime.now().isoformat(),
            "analysis_type": "comprehensive_breakdown"
        }

    async def _execute_solution_phase(self, message: str, user_id: str, 
                                    analysis_result: Dict, 
                                    complexity_analysis: Dict) -> Dict[str, Any]:
        """Execute solution development phase"""
        
        # Choose best model for solution based on analysis
        solution_model = self._select_solution_model(analysis_result, complexity_analysis)
        
        solution_prompt = self._generate_solution_prompt(
            message, analysis_result["content"], complexity_analysis
        )
        
        solution_result = await self.ai_client.call_ai(
            solution_model,
            solution_prompt,
            {"phase": "solution", "based_on_analysis": True}
        )
        
        return {
            "phase": "solution",
            "model": solution_model,
            "content": solution_result.get("response", ""),
            "confidence": solution_result.get("confidence", 0.7),
            "timestamp": datetime.now().isoformat(),
            "solution_type": "comprehensive_implementation",
            "based_on_analysis": analysis_result["content"][:200] + "..."
        }

    async def _execute_validation_phase(self, solution_result: Dict, 
                                      quality_requirements: Dict) -> Dict[str, Any]:
        """Execute quality validation phase"""
        
        # Use different model for validation
        validation_model = self._get_validation_model(solution_result["model"])
        
        validation_prompt = self._generate_validation_prompt(
            solution_result["content"], quality_requirements
        )
        
        validation_result = await self.ai_client.call_ai(
            validation_model,
            validation_prompt,
            {"phase": "validation", "quality_check": True}
        )
        
        return {
            "phase": "validation",
            "model": validation_model,
            "content": validation_result.get("response", ""),
            "confidence": validation_result.get("confidence", 0.7),
            "timestamp": datetime.now().isoformat(),
            "validation_type": "quality_assurance",
            "quality_score": self._calculate_quality_score(validation_result)
        }

    def _generate_analysis_prompt(self, message: str, complexity_analysis: Dict) -> str:
        """Generate comprehensive analysis prompt"""
        return f"""
        Perform a comprehensive analysis of the following request:
        
        Request: {message}
        
        Complexity Level: {complexity_analysis.get('complexity_tier', 'unknown')}
        Complexity Score: {complexity_analysis.get('overall_complexity', 0):.2f}
        
        Please provide:
        1. Detailed breakdown of requirements
        2. Identification of key challenges
        3. Analysis of different approaches
        4. Risk assessment
        5. Resource requirements estimation
        
        Focus on depth and comprehensiveness.
        """

    def _generate_solution_prompt(self, original_message: str, analysis: str, 
                                complexity_analysis: Dict) -> str:
        """Generate solution development prompt"""
        return f"""
        Based on the following analysis, provide a comprehensive solution:
        
        Original Request: {original_message}
        
        Analysis Results: {analysis}
        
        Complexity Level: {complexity_analysis.get('complexity_tier', 'unknown')}
        
        Please provide:
        1. Detailed implementation plan
        2. Step-by-step execution guide
        3. Code examples (if applicable)
        4. Best practices and recommendations
        5. Potential pitfalls and how to avoid them
        
        Ensure the solution is actionable and comprehensive.
        """

    def _generate_validation_prompt(self, solution: str, quality_requirements: Dict) -> str:
        """Generate validation prompt"""
        return f"""
        Please validate the following solution for quality and completeness:
        
        Solution: {solution}
        
        Quality Requirements: {quality_requirements}
        
        Please assess:
        1. Completeness of the solution
        2. Technical accuracy
        3. Clarity and actionability
        4. Potential issues or gaps
        5. Suggestions for improvement
        
        Provide a quality score (1-10) and detailed feedback.
        """

    def _select_solution_model(self, analysis_result: Dict, complexity_analysis: Dict) -> str:
        """Select best model for solution phase"""
        complexity_tier = complexity_analysis.get("complexity_tier", "basic_complex")
        
        # Advanced model selection logic
        if "code" in analysis_result.get("content", "").lower():
            return "gpt"  # Good for code solutions
        elif complexity_tier == "expert_research":
            return "claude"  # Best for complex research
        elif "strategy" in analysis_result.get("content", "").lower():
            return "gemini"  # Good for strategic solutions
        else:
            return "claude"  # Default for complex solutions

    def _get_validation_model(self, solution_model: str) -> str:
        """Get different model for validation"""
        validation_matrix = {
            "gpt": "claude",
            "claude": "gemini",
            "gemini": "gpt"
        }
        return validation_matrix.get(solution_model, "claude")

    def _calculate_quality_score(self, validation_result: Dict) -> float:
        """Calculate quality score from validation"""
        content = validation_result.get("response", "")
        
        # Extract quality score if mentioned
        import re
        score_match = re.search(r'(?:score|rating).*?(\d+(?:\.\d+)?)', content.lower())
        if score_match:
            score = float(score_match.group(1))
            if score <= 10:
                return score / 10  # Normalize to 0-1
        
        # Fallback quality assessment
        confidence = validation_result.get("confidence", 0.7)
        return confidence

    def _compile_advanced_response(self, accumulated_context: List[Dict]) -> Dict[str, Any]:
        """Compile final response from all phases"""
        
        # Find the solution phase
        solution_phase = next((ctx for ctx in accumulated_context if ctx["phase"] == "solution"), None)
        validation_phase = next((ctx for ctx in accumulated_context if ctx["phase"] == "validation"), None)
        
        if not solution_phase:
            # Emergency fallback
            return {
                "content": "Processing completed with limited results",
                "primary_model": "system",
                "overall_confidence": 0.5,
                "quality_metrics": {}
            }
        
        # Compile comprehensive response
        final_content = solution_phase["content"]
        
        # Add validation insights if available
        if validation_phase:
            validation_insights = validation_phase["content"]
            if "improvement" in validation_insights.lower() or "suggestion" in validation_insights.lower():
                final_content += f"\n\n**Quality Review Insights:**\n{validation_insights}"
        
        # Calculate overall confidence
        confidences = [ctx.get("confidence", 0.7) for ctx in accumulated_context]
        overall_confidence = sum(confidences) / len(confidences)
        
        # Quality metrics
        quality_metrics = {
            "phases_completed": len(accumulated_context),
            "validation_performed": validation_phase is not None,
            "quality_score": validation_phase.get("quality_score", 0.7) if validation_phase else 0.7,
            "comprehensiveness_score": min(1.0, len(final_content) / 1000)  # Based on response length
        }
        
        return {
            "content": final_content,
            "primary_model": solution_phase["model"],
            "overall_confidence": overall_confidence,
            "quality_metrics": quality_metrics,
            "compilation_successful": True
        }

    async def get_advanced_system_metrics(self) -> Dict[str, Any]:
        """
        Get advanced system metrics
        ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡πÉ‡∏´‡∏°‡πà‡πÑ‡∏°‡πà‡πÅ‡∏ï‡∏∞‡∏Ç‡∏≠‡∏á‡πÄ‡∏î‡∏¥‡∏°
        """
        # Get basic metrics from existing system
        basic_status = await self.get_system_status()
        
        # Add advanced metrics
        advanced_metrics = {
            "basic_system_status": basic_status,
            "advanced_orchestration": {
                "complex_workflows_active": self._count_active_complex_workflows(),
                "multi_phase_success_rate": self._calculate_multi_phase_success_rate(),
                "average_workflow_duration": self._calculate_avg_workflow_duration(),
                "quality_validation_rate": self._calculate_quality_validation_rate()
            },
            "ai_coordination": {
                "model_utilization_balance": self._calculate_model_utilization_balance(),
                "cross_model_validation_rate": self._calculate_cross_validation_rate(),
                "workflow_optimization_score": self._calculate_workflow_optimization()
            },
            "performance_intelligence": {
                "complexity_detection_accuracy": self._calculate_complexity_accuracy(),
                "adaptive_routing_effectiveness": self._calculate_adaptive_routing(),
                "resource_optimization_score": self._calculate_resource_optimization()
            },
            "predictive_metrics": {
                "workload_prediction_accuracy": self._calculate_workload_prediction(),
                "bottleneck_prediction": self._predict_system_bottlenecks(),
                "scaling_recommendations": self._generate_scaling_recommendations()
            }
        }
        
        return advanced_metrics

    # Helper methods for advanced metrics
    def _count_active_complex_workflows(self) -> int:
        """Count currently active complex workflows"""
        # Placeholder - implement based on actual workflow tracking
        return 3

    def _calculate_multi_phase_success_rate(self) -> float:
        """Calculate success rate of multi-phase workflows"""
        return 0.87

    def _calculate_avg_workflow_duration(self) -> float:
        """Calculate average duration of complex workflows (seconds)"""
        return 15.5

    def _calculate_quality_validation_rate(self) -> float:
        """Calculate rate of quality validation usage"""
        return 0.65

    def _calculate_model_utilization_balance(self) -> Dict[str, float]:
        """Calculate how balanced model utilization is"""
        return {
            "gpt": 0.35,
            "claude": 0.40,
            "gemini": 0.25,
            "balance_score": 0.83
        }

    def _calculate_cross_validation_rate(self) -> float:
        """Calculate cross-model validation usage"""
        return 0.42

    def _calculate_workflow_optimization(self) -> float:
        """Calculate workflow optimization effectiveness"""
        return 0.79

    async def execute_parallel_processing(self, message: str, user_id: str, 
                                    models: List[str] = None) -> Dict[str, Any]:
        if not models:
            models = ["gpt", "claude", "gemini"]

        start_time = datetime.now()

        # Create parallel tasks
        parallel_tasks = []
        for model in models:
            task = self._execute_model_task(model, message, user_id)
            parallel_tasks.append(task)

        # Execute all tasks concurrently
        try:
            results = await asyncio.gather(*parallel_tasks, return_exceptions=True)
            
            # Process results
            successful_results = []
            failed_results = []
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    failed_results.append({
                        "model": models[i],
                        "error": str(result),
                        "success": False
                    })
                else:
                    successful_results.append({
                        "model": models[i],
                        "result": result,
                        "success": True
                    })
            
            # Select best result
            best_result = self._select_best_parallel_result(successful_results)
            
            # Compile consensus if multiple good results
            consensus_result = self._generate_consensus_result(successful_results)
            
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            return {
                "response": best_result.get("content", consensus_result.get("content", "")),
                "model_used": "parallel_consensus",
                "primary_model": best_result.get("model", "unknown"),
                "confidence": consensus_result.get("confidence", 0.7),
                "parallel_results": {
                    "successful_models": [r["model"] for r in successful_results],
                    "failed_models": [r["model"] for r in failed_results],
                    "consensus_confidence": consensus_result.get("confidence", 0.7),
                    "result_variance": self._calculate_result_variance(successful_results)
                },
                "processing_time_ms": processing_time,
                "execution_type": "parallel_multi_model"
            }
            
        except Exception as e:
            logger.error(f"Parallel processing failed: {e}")
            # Fallback to standard processing: ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á error response ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
            return self._create_error_response(
                f"Parallel processing failed: {str(e)}", 
                user_id, 
                f"parallel_fail_{int(datetime.now().timestamp())}", 
                datetime.now()
            )

    async def _execute_model_task(self, model: str, message: str, user_id: str) -> Dict[str, Any]:
        """Execute single model task for parallel processing"""
        try:
            result = await self.ai_client.call_ai(
                model, 
                message, 
                {"execution_mode": "parallel", "user_id": user_id}
            )
            
            return {
                "model": model,
                "content": result.get("response", ""),
                "confidence": result.get("confidence", 0.7),
                "response_time": result.get("response_time", 0),
                "success": True
            }
        except Exception as e:
            raise Exception(f"Model {model} failed: {str(e)}")

    def _select_best_parallel_result(self, successful_results: List[Dict]) -> Dict[str, Any]:
        """Select best result from parallel execution"""
        if not successful_results:
            return {"content": "No successful results", "model": "none"}
        
        # Score results based on confidence and response quality
        scored_results = []
        for result in successful_results:
            model_result = result["result"]
            score = (
                model_result.get("confidence", 0.7) * 0.6 +  # Confidence weight
                min(1.0, len(model_result.get("content", "")) / 500) * 0.3 +  # Length weight
                (1.0 if model_result.get("response_time", 5000) < 3000 else 0.5) * 0.1  # Speed weight
            )
            scored_results.append({
                "result": model_result,
                "model": result["model"],
                "score": score
            })
        
        # Return highest scoring result
        best = max(scored_results, key=lambda x: x["score"])
        return {
            "content": best["result"].get("content", ""),
            "model": best["model"],
            "confidence": best["result"].get("confidence", 0.7),
            "selection_score": best["score"]
        }

    def _generate_consensus_result(self, successful_results: List[Dict]) -> Dict[str, Any]:
        """Generate consensus from multiple results"""
        if len(successful_results) < 2:
            if successful_results:
                result = successful_results[0]["result"]
                return {
                    "content": result.get("content", ""),
                    "confidence": result.get("confidence", 0.7),
                    "consensus_type": "single_result"
                }
            else:
                return {
                    "content": "No consensus possible",
                    "confidence": 0.3,
                    "consensus_type": "no_results"
                }
        
        # Simple consensus: average confidence, combine key insights
        confidences = [r["result"].get("confidence", 0.7) for r in successful_results]
        avg_confidence = sum(confidences) / len(confidences)
        
        # Extract common themes (simplified)
        all_content = [r["result"].get("content", "") for r in successful_results]
        consensus_content = f"Consensus from {len(successful_results)} models: "
        
        # Take the longest response as base (simplified consensus)
        longest_content = max(all_content, key=len) if all_content else ""
        consensus_content += longest_content
        
        return {
            "content": consensus_content,
            "confidence": avg_confidence,
            "consensus_type": "multi_model_average",
            "participating_models": [r["model"] for r in successful_results]
        }

    def _calculate_result_variance(self, results: List[Dict]) -> float:
        """Calculate variance in results"""
        if len(results) < 2:
            return 0.0
        
        confidences = [r["result"].get("confidence", 0.7) for r in results]
        lengths = [len(r["result"].get("content", "")) for r in results]
        
        # Simple variance calculation
        confidence_variance = self._variance(confidences)
        length_variance = self._variance(lengths) / 10000  # Normalize
        
        return (confidence_variance + length_variance) / 2

    def _variance(self, values: List[float]) -> float:
        """Calculate variance of values"""
        if not values:
            return 0.0
        mean = sum(values) / len(values)
        return sum((x - mean) ** 2 for x in values) / len(values)
    def _create_fallback_response(self, message: str, model: str) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Fallback Response"""
        return {
            "response": f"Enhanced Logic Controller processed: {message}",
            "model_used": model,
            "confidence": 0.7,
            "service_type": "enhanced_fallback",
            "timestamp": datetime.now().isoformat()
        }

    def _create_cancelled_response(self, workflow_plan: Dict, reason: str) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Response ‡πÄ‡∏°‡∏∑‡πà‡∏≠ User ‡∏¢‡∏Å‡πÄ‡∏•‡∏¥‡∏Å"""
        return {
            "response": f"Workflow cancelled: {reason}",
            "model_used": "workflow_cancelled",
            "confidence": 0.0,
            "orchestration_type": "cancelled",
            "workflow_used": False,
            "cancellation_reason": reason,
            "timestamp": datetime.now().isoformat()
        }

    def _create_error_response(self, error_msg: str, user_id: str, workflow_id: str, start_time: datetime) -> Dict[str, Any]:
        """‡∏™‡∏£‡πâ‡∏≤‡∏á Error Response"""
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return {
            "response": f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢ ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö Enhanced Logic Controller\n\n‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: {error_msg}\nWorkflow ID: {workflow_id}",
            "model_used": "enhanced_logic_error",
            "error": error_msg,
            "confidence": 0.0,
            "processing_time_seconds": processing_time,
            "workflow_id": workflow_id,
            "user_id": user_id,
            "orchestration_type": "error",
            "workflow_used": False,
            "timestamp": datetime.now().isoformat()
        }