# run_all_tests.py
"""
Test runner script for all NAVA components
Run this script to execute all tests and generate a comprehensive report
"""

import os
import sys
import subprocess
import time
from datetime import datetime
import json

def run_test_file(test_file_path, test_name):
    """Run a specific test file and return results"""
    print(f"\n{'='*60}")
    print(f"ğŸ§ª Running {test_name}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    try:
        # Run pytest with verbose output
        result = subprocess.run([
            sys.executable, "-m", "pytest", 
            test_file_path, 
            "-v", 
            "--tb=short",
            "--disable-warnings"
        ], capture_output=True, text=True, timeout=300)
        
        execution_time = time.time() - start_time
        
        success = result.returncode == 0
        
        print(f"â±ï¸  Execution time: {execution_time:.2f} seconds")
        print(f"ğŸ“Š Exit code: {result.returncode}")
        
        if success:
            print(f"âœ… {test_name} - PASSED")
        else:
            print(f"âŒ {test_name} - FAILED")
            print("\nğŸ” Error output:")
            print(result.stderr)
            print("\nğŸ“ Test output:")
            print(result.stdout)
        
        return {
            "name": test_name,
            "file": test_file_path,
            "success": success,
            "execution_time": execution_time,
            "output": result.stdout,
            "errors": result.stderr,
            "exit_code": result.returncode
        }
        
    except subprocess.TimeoutExpired:
        print(f"â° {test_name} - TIMEOUT (exceeded 5 minutes)")
        return {
            "name": test_name,
            "file": test_file_path,
            "success": False,
            "execution_time": 300.0,
            "output": "",
            "errors": "Test execution timeout",
            "exit_code": -1
        }
    except Exception as e:
        print(f"ğŸ’¥ {test_name} - ERROR: {str(e)}")
        return {
            "name": test_name,
            "file": test_file_path,
            "success": False,
            "execution_time": time.time() - start_time,
            "output": "",
            "errors": str(e),
            "exit_code": -2
        }

def check_dependencies():
    """Check if required dependencies are available"""
    print("ğŸ” Checking dependencies...")
    
    required_packages = ["pytest", "pydantic", "pillow"]
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
            print(f"âœ… {package} - available")
        except ImportError:
            print(f"âŒ {package} - missing")
            missing_packages.append(package)
    
    if missing_packages:
        print(f"\nâš ï¸  Missing packages: {', '.join(missing_packages)}")
        print("Install with: pip install " + " ".join(missing_packages))
        return False
    
    return True

def create_test_directories():
    """Create test directories if they don't exist"""
    test_dirs = [
        "backend/services/01-core/nava-logic-controller/tests",
        "backend/services/03-external-ai/gemini-client/tests", 
        "backend/services/shared/common/tests"
    ]
    
    for test_dir in test_dirs:
        os.makedirs(test_dir, exist_ok=True)
        
        # Create __init__.py files
        init_file = os.path.join(test_dir, "__init__.py")
        if not os.path.exists(init_file):
            with open(init_file, "w") as f:
                f.write("# Test package\n")

def generate_test_report(test_results):
    """Generate a comprehensive test report"""
    total_tests = len(test_results)
    passed_tests = sum(1 for result in test_results if result["success"])
    failed_tests = total_tests - passed_tests
    total_time = sum(result["execution_time"] for result in test_results)
    
    success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
    
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ NAVA TEST EXECUTION REPORT")
    print(f"{'='*80}")
    print(f"ğŸ“… Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸  Total execution time: {total_time:.2f} seconds")
    print(f"ğŸ“Š Tests executed: {total_tests}")
    print(f"âœ… Tests passed: {passed_tests}")
    print(f"âŒ Tests failed: {failed_tests}")
    print(f"ğŸ“ˆ Success rate: {success_rate:.1f}%")
    
    if success_rate >= 90:
        print(f"ğŸ‰ EXCELLENT - Ready for deployment!")
    elif success_rate >= 80:
        print(f"ğŸ‘ GOOD - Minor issues to address")
    elif success_rate >= 70:
        print(f"âš ï¸  NEEDS IMPROVEMENT - Several issues found")
    else:
        print(f"ğŸš¨ CRITICAL - Major issues require attention")
    
    print(f"\n{'='*80}")
    print(f"ğŸ“ DETAILED RESULTS")
    print(f"{'='*80}")
    
    for i, result in enumerate(test_results, 1):
        status = "âœ… PASS" if result["success"] else "âŒ FAIL"
        print(f"{i:2d}. {result['name']:<30} {status} ({result['execution_time']:.2f}s)")
        
        if not result["success"]:
            print(f"    âš ï¸  Error: {result['errors'][:100]}...")
    
    # Save detailed report to file
    report_data = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": success_rate,
            "total_time": total_time
        },
        "detailed_results": test_results
    }
    
    with open("test_report.json", "w") as f:
        json.dump(report_data, f, indent=2)
    
    print(f"\nğŸ’¾ Detailed report saved to: test_report.json")
    
    return success_rate >= 80  # Return True if ready for deployment

def main():
    """Main test execution function"""
    print("ğŸš€ NAVA Comprehensive Test Suite")
    print("=" * 60)
    
    # Check dependencies
    if not check_dependencies():
        print("\nâŒ Dependencies check failed. Please install missing packages.")
        return False
    
    # Create test directories
    create_test_directories()
    
    # Define all test files to run
    test_files = [
        {
            "name": "Enhanced Chat Models",
            "file": "backend/services/01-core/nava-logic-controller/tests/test_enhanced_chat.py"
        },
        {
            "name": "Feedback System", 
            "file": "backend/services/01-core/nava-logic-controller/tests/test_feedback.py"
        },
        {
            "name": "Multimodal Handler",
            "file": "backend/services/03-external-ai/gemini-client/tests/test_multimodal_handler.py"
        },
        {
            "name": "Memory Manager",
            "file": "backend/services/shared/common/tests/test_memory_manager.py"
        },
        {
            "name": "Trust Calculator",
            "file": "backend/services/shared/common/tests/test_trust_calculator.py"
        }
    ]
    
    # Check if test files exist
    print("ğŸ“‚ Checking test files...")
    missing_files = []
    for test in test_files:
        if os.path.exists(test["file"]):
            print(f"âœ… {test['file']}")
        else:
            print(f"âŒ {test['file']} - NOT FOUND")
            missing_files.append(test["file"])
    
    if missing_files:
        print(f"\nâš ï¸  Missing test files: {len(missing_files)}")
        print("Please create the missing test files before running the test suite.")
        return False
    
    # Run all tests
    print(f"\nğŸ¯ Starting test execution for {len(test_files)} test suites...")
    test_results = []
    
    start_time = time.time()
    
    for test_info in test_files:
        result = run_test_file(test_info["file"], test_info["name"])
        test_results.append(result)
    
    total_execution_time = time.time() - start_time
    
    # Generate and display report
    deployment_ready = generate_test_report(test_results)
    
    print(f"\n{'='*80}")
    print(f"ğŸ Test suite completed in {total_execution_time:.2f} seconds")
    
    if deployment_ready:
        print(f"ğŸ¯ DEPLOYMENT READY - All critical tests passed!")
        print(f"âœ… You can proceed with deployment")
        return True
    else:
        print(f"ğŸ”§ NEEDS FIXES - Please address failing tests before deployment")
        print(f"âŒ Do not deploy until issues are resolved")
        return False

if __name__ == "__main__":
    # Set up Python path to find modules
    current_dir = os.path.dirname(os.path.abspath(__file__))
    sys.path.insert(0, current_dir)
    
    # Change to the project root directory if needed
    if os.path.basename(current_dir) != "nava-projects":
        # Try to find the project root
        project_root = current_dir
        while project_root != "/" and not os.path.exists(os.path.join(project_root, "backend")):
            project_root = os.path.dirname(project_root)
        
        if os.path.exists(os.path.join(project_root, "backend")):
            os.chdir(project_root)
            print(f"ğŸ“ Changed to project root: {project_root}")
    
    # Run the test suite
    try:
        success = main()
        exit_code = 0 if success else 1
        sys.exit(exit_code)
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Test execution interrupted by user")
        sys.exit(130)
    except Exception as e:
        print(f"\n\nğŸ’¥ Unexpected error: {str(e)}")
        sys.exit(1)
